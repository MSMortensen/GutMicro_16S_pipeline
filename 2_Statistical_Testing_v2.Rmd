---
title: "CEFTA Microbiome Statistical testing"
author: "masmo"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    toc_depth: 4
    collapsed: false
    code_folding: hide
    number_sections: true
knit: (function(inputFile, encoding) { 
    rmarkdown::render(
        inputFile, encoding = encoding,
        output_dir = file.path(dirname(inputFile),"output"),
        output_file = paste0("CEFTA_", Sys.Date(), "_StatisticalTesting.html")) 
    })
---

```{r setup, eval=TRUE, echo=FALSE, message=FALSE,warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load libraries
library(tidyverse)
library(phyloseq)
library(ggpubr)
library(rstatix)
library(kableExtra)
library(picante)

# Create used folders if missing
if (!file.exists("R_objects")) dir.create(file.path(getwd(), "R_objects"))
if (!file.exists("plots")) dir.create(file.path(getwd(), "plots"))
if (!file.exists("tables")) dir.create(file.path(getwd(), "tables"))
if (!file.exists("scripts")) dir.create(file.path(getwd(), "scripts"))

```

# STATISTICAL TESTING {.tabset .tabset-fade .tabset-pills}

This Rmarkdown should be used to explore the metadata for a project, identify the relevant statistical tests to run on which variables, as well as perform those tests. The theory is copied from "[Choosing the Right Statistical Test \| Types & Examples (scribbr.com)](https://www.scribbr.com/statistics/statistical-tests/)" with the methods following the included recommendations.

This document can also be used to test all non microbiome data.

# THEORY

### **What does a statistical test do?**

Statistical tests work by calculating a **test statistic** -- a number that describes how much the relationship between variables in your test differs from the null hypothesis of no relationship.

It then calculates a **p-value** (probability value). The p-value estimates how likely it is that you would see the difference described by the test statistic if the null hypothesis of no relationship were true.

If the value of the test statistic is more extreme than the statistic calculated from the null hypothesis, then you can infer a **statistically significant relationship** between the predictor and outcome variables.

If the value of the test statistic is less extreme than the one calculated from the null hypothesis, then you can infer **no statistically significant relationship** between the predictor and outcome variables.

### **When to perform a statistical test**

You can perform statistical tests on data that have been collected in a statistically valid manner -- either through an experiment, or through observations made using probability sampling methods.

For a statistical test to be valid, your sample size needs to be large enough to approximate the true distribution of the population being studied.

To determine which statistical test to use, you need to know:

-   whether your data meets certain assumptions.

-   the types of variables that you're dealing with.

### **Statistical assumptions**

Statistical tests make some common assumptions about the data they are testing:

1.  **Independence of observations** (a.k.a. no autocorrelation): The observations/variables you include in your test are not related (for example, multiple measurements of a single test subject are not independent, while measurements of multiple different test subjects are independent).

2.  **Homogeneity of variance**: the variance within each group being compared is similar among all groups. If one group has much more variation than others, it will limit the test's effectiveness.

3.  **Normality of data:** the data follows a normal distribution (a.k.a. a bell curve). This assumption applies only to quantitative data.

If your data do not meet the assumptions of normality or homogeneity of variance, you may be able to perform a **nonparametric statistical test**, which allows you to make comparisons without any assumptions about the data distribution.

If your data do not meet the assumption of independence of observations, you may be able to use a test that accounts for structure in your data (repeated-measures tests or tests that include blocking variables).

### **Types of variables**

The [types of variables](https://www.scribbr.com/methodology/types-of-variables/) you have usually determine what type of statistical test you can use.

**Quantitative variables** represent amounts of things (e.g. the number of trees in a forest). Types of quantitative variables include:

-   **Continuous** (a.k.a ratio variables): represent measures and can usually be divided into units smaller than one (e.g. 0.75 grams).

-   **Discrete** (a.k.a integer variables): represent counts and usually can't be divided into units smaller than one (e.g. 1 tree).

**Categorical variables** represent groupings of things (e.g. the different tree species in a forest). Types of categorical variables include:

-   **Ordinal**: represent data with an order (e.g. rankings).

-   **Nominal**: represent group names (e.g. brands or species names).

-   **Binary**: represent data with a yes/no or 1/0 outcome (e.g. win or lose).

Choose the test that fits the types of predictor and outcome variables you have collected. Consult the tables below to see which test best matches your variables.

### **Choosing the right test** {.tabset .tabset-fade .tabset-pills}

Parametric tests usually have stricter requirements than nonparametric tests, and are able to make stronger inferences from the data. They can only be conducted with data that adheres to the common assumptions of statistical tests.

Use the flowchart to choose the best parametric test for your data, then test that your data fullfill the statistical assumptions. If your data does not fulfill the assumptions, then choose the appropriate non-parametric test. More information are listed in the tabs below.

![flowchart for choosing a statistical test](flowchart-for-choosing-a-statistical-test.png)\

# LOAD DATA

To ease loading data we will import it here and save it in a predefined file that will be imported throughout this script. If continuing directly from **1_Import_QC.RData** data can be extracted directly from the phyloseq object of choice (first part of the code block) or imported from any external file using the import function (second part of the code block). *!Delete the option that you do not use!*. The third part of the code block stores the data in a file for easy import later on.

```{r load_data, eval = TRUE}

##### FIRST PART
# Load phyloseq metadata
load("R_objects/Phyloseq_harsh.Rdata")
dat <- as_tibble(sample_data(phy))

##### SECOND PART
# Load data from table using readr (File > Import Dataset > From Text (readr)...) and paste the code here
dat <- read_delim("...", delim = "\t", escape_double = FALSE, trim_ws = TRUE)

#### THIRD PART
save(dat, file = "R_objects/stat_test_dat.RData")
```


#### **Regression tests**

Regression tests look for **cause-and-effect relationships**. They can be used to estimate the effect of one or more continuous variables on another variable.

+----------------------------------------------------------------------------------------------+--------------------------+------------------+----------------------------------------------------------------------------+
|                                                                                              | Predictor variable       | Outcome variable | Research question example                                                  |
+:=============================================================================================+:========================:+:================:+:==========================================================================:+
| [Simple linear regression](https://www.scribbr.com/statistics/simple-linear-regression/)     | -   Continuous           | -   Continuous   | What is the effect of income on longevity?                                 |
|                                                                                              |                          |                  |                                                                            |
|                                                                                              | -   1 predictor          | -   1 outcome    |                                                                            |
+----------------------------------------------------------------------------------------------+--------------------------+------------------+----------------------------------------------------------------------------+
| [Multiple linear regression](https://www.scribbr.com/statistics/multiple-linear-regression/) | -   Continuous           | -   Continuous   | What is the effect of income and minutes of exercise per day on longevity? |
|                                                                                              |                          |                  |                                                                            |
|                                                                                              | -   2 or more predictors | -   1 outcome    |                                                                            |
+----------------------------------------------------------------------------------------------+--------------------------+------------------+----------------------------------------------------------------------------+
| Logistic regression                                                                          | -   Continuous           | -   Binary       | What is the effect of drug dosage on the survival of a test subject?       |
+----------------------------------------------------------------------------------------------+--------------------------+------------------+----------------------------------------------------------------------------+

#### **Comparison tests**

Comparison tests look for **differences among group means**. They can be used to test the effect of a categorical variable on the [mean value](https://www.scribbr.com/statistics/mean/) of some other characteristic.

[T-tests](https://www.scribbr.com/statistics/t-test/) are used when comparing the means of precisely two groups (e.g. the average heights of men and women). [ANOVA](https://www.scribbr.com/statistics/one-way-anova/) and MANOVA tests are used when comparing the means of more than two groups (e.g. the average heights of children, teenagers, and adults).

+--------------------+-------------------------+--------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
|                    | Predictor variable      | Outcome variable                           | Research question example                                                                                           |
+:===================+:=======================:+:==========================================:+:===================================================================================================================:+
| Paired t-test      | -   Categorical         | -   Quantitative                           | What is the effect of two different test prep programs on the average exam scores for students from the same class? |
|                    |                         |                                            |                                                                                                                     |
|                    | -   1 predictor         | -   groups come from the same population   |                                                                                                                     |
+--------------------+-------------------------+--------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| Independent t-test | -   Categorical         | -   Quantitative                           | What is the difference in average exam scores for students from two different schools?                              |
|                    |                         |                                            |                                                                                                                     |
|                    | -   1 predictor         | -   groups come from different populations |                                                                                                                     |
+--------------------+-------------------------+--------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| ANOVA              | -   Categorical         | -   Quantitative                           | What is the difference in average pain levels among post-surgical patients given three different painkillers?       |
|                    |                         |                                            |                                                                                                                     |
|                    | -   1 or more predictor | -   1 outcome                              |                                                                                                                     |
+--------------------+-------------------------+--------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| MANOVA             | -   Categorical         | -   Quantitative                           | What is the effect of flower species on petal length, petal width, and stem length?                                 |
|                    |                         |                                            |                                                                                                                     |
|                    | -   1 or more predictor | -   2 or more outcome                      |                                                                                                                     |
+--------------------+-------------------------+--------------------------------------------+---------------------------------------------------------------------------------------------------------------------+

#### **Correlation tests**

[Correlation tests](https://www.scribbr.com/statistics/correlation-coefficient/) **check whether variables are related** without hypothesizing a cause-and-effect relationship.

These can be used to test whether two variables you want to use in (for example) a multiple regression test are autocorrelated.

+---------------+----------------------------+-------------------------------------------+
|               | Variables                  | Research question example                 |
+:==============+:==========================:+:=========================================:+
| Pearson's *r* | -   2 continuous variables | How are latitude and temperature related? |
+---------------+----------------------------+-------------------------------------------+

#### **Nonparametric test alternatives**

Non-parametric tests don't make as many assumptions about the data, and are useful when one or more of the common statistical assumptions are violated. However, the inferences they make aren't as strong as with parametric tests.

+---------------------------------+----------------------+--------------------------------------------+---------------------+
|                                 | Predictor variable   | Outcome variable                           | Use in place of...  |
+:================================+:====================:+:==========================================:+:===================:+
| Spearman's *r*                  | -   Quantitative     | -   Quantitative                           | Pearson's *r*       |
+---------------------------------+----------------------+--------------------------------------------+---------------------+
| Chi square test of independence | -   Categorical      | -   Categorical                            | Pearson's *r*       |
+---------------------------------+----------------------+--------------------------------------------+---------------------+
| Sign test                       | -   Categorical      | -   Quantitative                           | One-sample *t*-test |
+---------------------------------+----------------------+--------------------------------------------+---------------------+
| Kruskal--Wallis *H*             | -   Categorical      | -   Quantitative                           | ANOVA               |
|                                 |                      |                                            |                     |
|                                 | -   3 or more groups |                                            |                     |
+---------------------------------+----------------------+--------------------------------------------+---------------------+
| ANOSIM                          | -   Categorical      | -   Quantitative                           | MANOVA              |
|                                 |                      |                                            |                     |
|                                 | -   3 or more groups | -   2 or more outcome variables            |                     |
+---------------------------------+----------------------+--------------------------------------------+---------------------+
| Wilcoxon Rank-Sum test          | -   Categorical      | -   Quantitative                           | Independent t-test  |
|                                 |                      |                                            |                     |
|                                 | -   2 groups         | -   groups come from different populations |                     |
+---------------------------------+----------------------+--------------------------------------------+---------------------+
| Wilcoxon Signed-rank test       | -   Categorical      | -   Quantitative                           | Paired t-test       |
|                                 |                      |                                            |                     |
|                                 | -   2 groups         | -   groups come from the same population   |                     |
+---------------------------------+----------------------+--------------------------------------------+---------------------+



## TEST STATISTICAL ASSUMPTIONS

### Independence of observations

First we have to ensure that the variables we are testing are independent of each other.

#### Categorical variables

Categorical variables should be compared using **Chi squared test of independence**. When testing categorical data it is important that they are formatted as factors. This is especially true for variables that can be misinterpreted as being numerical.

```{r cat_var_cor, eval = TRUE}

# This converts all character columns to factor (if only specific columns should be converted replace "where(is_character)" with "c(<col1>,<col2>,...)"
dat <- dat %>% mutate(across(where(is_character), as_factor))

# Create vector of categorical variables
cat_var <- dat %>% 
  select_if(is.factor) %>% 
  select(where(~n_distinct(.) > 1)) %>% 
  select(where(~n_distinct(.) < (nrow(dat)/2))) %>%
  colnames()

length(cat_var) # If less than 2 variables the following code should not be run

# Test the variables pairwise 
cat_var %>% 
  combn(2) %>% 
  t() %>% 
  as_tibble() %>% 
  rowwise %>% 
  mutate(chisq_test = list(
    table(dat[[V1]], dat[[V2]]) %>% chisq.test()
    ),
    chisq_pval = chisq_test$p.value
    )

```

If any correlations are significant, both variables cannot be included in the same test

#### Quantitative variables

Quantitative variables should first be tested to see if it follows a normal distribution, followed by **Pearson's r** or **Spearman's r**

```{r con_var_cor, eval = TRUE}

# Select variables
num_var <- dat %>% select_if(is.numeric) %>% select(where(~n_distinct(.,na.rm = t) > 1)) %>% colnames()


# Determine if variables are normally distributed
dat %>% shapiro_test(Streptomycin_g_l)

```

### Homogeneity of variance

the variance within each group being compared is similar among all groups. If one group has much more variation than others, it will limit the test's effectiveness.

### Normality of data

:\*\* the data follows a normal distribution (a.k.a. a bell curve). This assumption applies only to quantitative data.

## CATEGORICAL VARIABLES EFFECTS {.tabset .tabset-fade .tabset-pills}

Keeping in mind possible batch effects, now we can test for project effects. Depending on the project it might be best to test each variable individually or to perform a nested test of the variables. It is important to ensure that R has interpreted the selected variable as a factor, charactor variables will automatically be interpreted as factors, but variables that consists solely of numbers (numerical or integers) is interpreted as continous by R and must be transformed to be factors.

### WEEK (INDIVIDUAL)

Here is code to test alpha diversity for a single categorical variable. To change the variable, just update the TEST.VAR

```{r alpha_div_Week_independence, eval=TRUE}

# Load data
load("R_objects/stat_test_dat.RData")
TEST.VAR <- "Week_clean"

# Remove samples with incomplete metadata
dat <- dat[!is.na(dat[,TEST.VAR]),]
dat[,TEST.VAR] <- as.factor(dat[,TEST.VAR])

### Determine if any batch effect might influence your data
# Calculate distribution counts
freq.t <- with(dat, table(Run, Week_clean, exclude = NULL))
freq.t

# Determine distribution percentages
prop.table(freq.t,2)*100

# Test if any difference is significant (if not significant the batch effect should be negligible)
chisq_test(freq.t)
```

As there is no significant interaction identified by the chi^2^-test, we can ignore the batch effects when analysing the variable.

```{r alpha_div_Week_test, eval=TRUE, fig.cap="Boxplot of alpha diversity"}
################################################################################

dat <- dat[dat$FMT == 1,]
#### Test project variable
### Observed richness
FORMULA <- as.formula(paste("Observed ~", TEST.VAR, sep = " "))
compare_means(FORMULA,  data = dat, method = "kruskal")

## If significant:
# Perform pairwise comparisons
stat.test <- dat %>%
  wilcox_test(FORMULA) %>%
  adjust_pvalue(method = "BH") %>%
  add_significance("p.adj") %>% 
  add_x_position(x = TEST.VAR) %>%
  p_format("p.adj", accuracy = 0.0001, trailing.zero = TRUE, new.col = TRUE)

# Format for 
if (sum(stat.test$p.adj.signif != "ns") == 0) {
  stat.sig <- stat.test %>%
    add_y_position(step.increase = 0.25) %>%
    mutate(y.position = seq(min(y.position), max(y.position),length.out = n()))
} else {
  stat.sig <- stat.test[stat.test$p.adj.signif != "ns",] %>%
    add_y_position(step.increase = 0.25) %>%
    mutate(y.position = seq(min(y.position), max(y.position),length.out = n()))
}

# Create plot
p <- ggboxplot(dat, x = TEST.VAR, y = "Observed",
          color = TEST.VAR, palette = "jco",
          add = "jitter")

p.obs <- p + stat_pvalue_manual(stat.sig, label = "p.adj.format",tip.length = 0)

### Test shannon diversity index
# Run statistical test of batch effect
FORMULA <- as.formula(paste("Shannon ~", TEST.VAR, sep = " "))
compare_means(FORMULA,  data = dat, method = "kruskal")

## If significant:
# Perform pairwise comparisons
stat.test <- dat %>%
  wilcox_test(FORMULA) %>%
  adjust_pvalue(method = "BH") %>%
  add_significance("p.adj") %>% 
  add_x_position(x = TEST.VAR) %>%
  p_format("p.adj", accuracy = 0.0001, trailing.zero = TRUE, new.col = TRUE)

# Format for 
if (sum(stat.test$p.adj.signif != "ns") == 0) {
  stat.sig <- stat.test %>%
    add_y_position(step.increase = 0.25) %>%
    mutate(y.position = seq(min(y.position), max(y.position),length.out = n()))
} else {
  stat.sig <- stat.test[stat.test$p.adj.signif != "ns",] %>%
    add_y_position(step.increase = 0.25) %>%
    mutate(y.position = seq(min(y.position), max(y.position),length.out = n()))
}
# Create plot
p <- ggboxplot(dat, x = TEST.VAR, y = "Shannon", color = TEST.VAR, palette = "jco", add = "jitter")

p.sdi <- p + stat_pvalue_manual(stat.sig, label = "p.adj.format",tip.length = 0)

# Test Faith phylogenetic distance
FORMULA <- as.formula(paste("FaithPD ~", TEST.VAR, sep = " "))
compare_means(FORMULA,  data = dat, method = "kruskal")

## If significant:
# Perform pairwise comparisons
stat.test <- dat %>%
  wilcox_test(FORMULA) %>%
  adjust_pvalue(method = "BH") %>%
  add_significance("p.adj") %>% 
  add_x_position(x = TEST.VAR) %>%
  p_format("p.adj", accuracy = 0.0001, trailing.zero = TRUE, new.col = TRUE)

# Format for 
if (sum(stat.test$p.adj.signif != "ns") == 0) {
  stat.sig <- stat.test %>%
    add_y_position(step.increase = 0.25) %>%
    mutate(y.position = seq(min(y.position), max(y.position),length.out = n()))
} else {
  stat.sig <- stat.test[stat.test$p.adj.signif != "ns",] %>%
    add_y_position(step.increase = 0.25) %>%
    mutate(y.position = seq(min(y.position), max(y.position),length.out = n()))
}

# Create plot
p <- ggboxplot(dat, x = TEST.VAR, y = "FaithPD",
          color = TEST.VAR, palette = "jco",
          add = "jitter")

p.fpd <- p + stat_pvalue_manual(stat.sig, label = "p.adj.format",tip.length = 0)

# If there is a significant batch effect, then it will be necessary to correct following tests for this effect.
filename <- paste0("plots/adiv_",TEST.VAR,".png")
adiv_plot <- ggarrange(p.obs,p.sdi, p.fpd, nrow = 1, labels = c("A)","B)","C)"), common.legend = TRUE,legend = "bottom")
adiv_plot + ggtitle(paste0("Difference in alpha diversity between ",TEST.VAR,":"))
suppressMessages(ggsave(filename = filename, plot = adiv_plot, device = "png"))

# clear the environment and release memory
rm(list = ls(all.names = TRUE))
invisible(gc())
```

### OUTCOME (INDIVIDUAL)

Here is code to test alpha diversity for a single variable.

```{r alpha_div_outcome_independence, eval=TRUE}

# Load data
load("R_objects/stat_test_dat.RData")
TEST.VAR <- "Outcome2"

# Remove samples with incomplete metadata
dat <- dat[!is.na(dat[,TEST.VAR]),]
dat[,TEST.VAR] <- as.factor(dat[,TEST.VAR])

### Determine if any batch effect might influence your data
# Calculate distribution counts
freq.t <- with(dat, table(Run, Week_clean, exclude = NULL))
freq.t

# Determine distribution percentages
prop.table(freq.t,2)*100

# Test if any difference is significant (if not significant the batch effect should be negligible)
chisq_test(freq.t)
```

As there is no significant interaction identified by the chi^2^-test, we can ignore the batch effects when analysing the variable.

```{r alpha_div_outcome_test, eval=TRUE, fig.cap="Boxplot of alpha diversity"}
################################################################################

#### Test project variable
### Observed richness
FORMULA <- as.formula(paste("Observed ~", TEST.VAR, sep = " "))
compare_means(FORMULA,  data = dat, method = "kruskal")

## If significant:
# Perform pairwise comparisons
stat.test <- dat %>%
  wilcox_test(FORMULA) %>%
  adjust_pvalue(method = "BH") %>%
  add_significance("p.adj") %>% 
  add_x_position(x = TEST.VAR) %>%
  p_format("p.adj", accuracy = 0.0001, trailing.zero = TRUE, new.col = TRUE)

# Format for 
if (sum(stat.test$p.adj.signif != "ns") == 0) {
  stat.sig <- stat.test %>%
    add_y_position(step.increase = 0.25) %>%
    mutate(y.position = seq(min(y.position), max(y.position),length.out = n()))
} else {
  stat.sig <- stat.test[stat.test$p.adj.signif != "ns",] %>%
    add_y_position(step.increase = 0.25) %>%
    mutate(y.position = seq(min(y.position), max(y.position),length.out = n()))
}

# Create plot
p <- ggboxplot(dat, x = TEST.VAR, y = "Observed",
          color = TEST.VAR, palette = "jco",
          add = "jitter")

p.obs <- p + stat_pvalue_manual(stat.sig, label = "p.adj.format",tip.length = 0)

### Test shannon diversity index
# Run statistical test of batch effect
FORMULA <- as.formula(paste("Shannon ~", TEST.VAR, sep = " "))
compare_means(FORMULA,  data = dat, method = "kruskal")

## If significant:
# Perform pairwise comparisons
stat.test <- dat %>%
  wilcox_test(FORMULA) %>%
  adjust_pvalue(method = "BH") %>%
  add_significance("p.adj") %>% 
  add_x_position(x = TEST.VAR) %>%
  p_format("p.adj", accuracy = 0.0001, trailing.zero = TRUE, new.col = TRUE)

# Format for 
if (sum(stat.test$p.adj.signif != "ns") == 0) {
  stat.sig <- stat.test %>%
    add_y_position(step.increase = 0.25) %>%
    mutate(y.position = seq(min(y.position), max(y.position),length.out = n()))
} else {
  stat.sig <- stat.test[stat.test$p.adj.signif != "ns",] %>%
    add_y_position(step.increase = 0.25) %>%
    mutate(y.position = seq(min(y.position), max(y.position),length.out = n()))
}
# Create plot
p <- ggboxplot(dat, x = TEST.VAR, y = "Shannon", color = TEST.VAR, palette = "jco", add = "jitter")

p.sdi <- p + stat_pvalue_manual(stat.sig, label = "p.adj.format",tip.length = 0)

# Test Faith phylogenetic distance
FORMULA <- as.formula(paste("FaithPD ~", TEST.VAR, sep = " "))
compare_means(FORMULA,  data = dat, method = "kruskal")

## If significant:
# Perform pairwise comparisons
stat.test <- dat %>%
  wilcox_test(FORMULA) %>%
  adjust_pvalue(method = "BH") %>%
  add_significance("p.adj") %>% 
  add_x_position(x = TEST.VAR) %>%
  p_format("p.adj", accuracy = 0.0001, trailing.zero = TRUE, new.col = TRUE)

# Format for 
if (sum(stat.test$p.adj.signif != "ns") == 0) {
  stat.sig <- stat.test %>%
    add_y_position(step.increase = 0.25) %>%
    mutate(y.position = seq(min(y.position), max(y.position),length.out = n()))
} else {
  stat.sig <- stat.test[stat.test$p.adj.signif != "ns",] %>%
    add_y_position(step.increase = 0.25) %>%
    mutate(y.position = seq(min(y.position), max(y.position),length.out = n()))
}

# Create plot
p <- ggboxplot(dat, x = TEST.VAR, y = "FaithPD",
          color = TEST.VAR, palette = "jco",
          add = "jitter")

p.fpd <- p + stat_pvalue_manual(stat.sig, label = "p.adj.format",tip.length = 0)

# If there is a significant batch effect, then it will be necessary to correct following tests for this effect.
filename <- paste0("plots/adiv_",TEST.VAR,".png")
adiv_plot <- ggarrange(p.obs,p.sdi, p.fpd, nrow = 1, labels = c("A)","B)","C)"), common.legend = TRUE,legend = "bottom")
adiv_plot + ggtitle(paste0("Difference in alpha diversity between ",TEST.VAR,":"))
suppressMessages(ggsave(filename = filename, plot = adiv_plot, device = "png"))

# clear the environment and release memory
rm(list = ls(all.names = TRUE))
invisible(gc())
```

### WEEK AND OUTCOME (NESTED)

Testing two variables are used when there is a nested aspect in the analysis, for example difference in treatment at each timepoint, that would have the timepoint as the outer variable and treatment as the inner variable

```{r adiv_nested, eval = FALSE}

# Load data
load("R_objects/stat_test_dat.RData")

dat$Week_factor <- as.factor(dat$Week_clean)
INNER.VAR <- "Outcome2"
OUTER.VAR <- "Week_clean"

# Remove samples with incomplete metadata
dat <- dat[!is.na(dat[,INNER.VAR]) & !is.na(dat[,OUTER.VAR]),]
dat[,OUTER.VAR] <- as.factor(dat[,OUTER.VAR])
dat[,INNER.VAR] <- as.factor(dat[,INNER.VAR])

#### Test project variable
### Observed richness
fit <- aov(as.formula(paste("Observed ~", OUTER.VAR,"*",INNER.VAR, sep = " ")), data = dat)
anova(fit)
TukeyHSD(fit)

## Calculate stats for inner variable
# Perform pairwise comparisons
stat.test <- dat %>%
  group_by(.data[[OUTER.VAR]]) %>%
  wilcox_test(as.formula(paste("Observed ~", INNER.VAR, sep = " "))) %>%
  adjust_pvalue(method = "BH") %>%
  add_significance("p.adj") %>% 
  add_xy_position(x = OUTER.VAR, dodge = 0.8) %>%
  p_format("p.adj", accuracy = 0.0001, trailing.zero = TRUE, new.col = TRUE)

## Calculate stats for outer variable
# Perform pairwise comparisons
stat.test2 <- dat %>%
  wilcox_test(as.formula(paste("Observed ~", OUTER.VAR, sep = " "))) %>%
  adjust_pvalue(method = "BH") %>%
  add_significance("p.adj") %>% 
  add_xy_position(x = OUTER.VAR) %>%
  p_format("p.adj", accuracy = 0.0001, trailing.zero = TRUE, new.col = TRUE)

# Adjust y value for outer p-values
stat.test2$y.position <- max(stat.test$y.position)*1.1

# Create plot
p <- ggboxplot(dat, x = OUTER.VAR, y = "Observed",
          color = INNER.VAR, palette = "jco",
          add = "jitter")

# Add p-values
p.obs <- p + stat_pvalue_manual(stat.test, label = "p.adj.format",tip.length = 0)
p.obs <- p.obs + stat_pvalue_manual(stat.test2, label = "p.adj.format",tip.length = 0.02, step.increase = 0.1)

### Shannon diversity index
fit <- aov(as.formula(paste("Shannon ~", OUTER.VAR,"*",INNER.VAR, sep = " ")), data = dat)
anova(fit)
TukeyHSD(fit)

## Calculate stats for inner variable
# Perform pairwise comparisons
stat.test <- dat %>%
  group_by(.data[[OUTER.VAR]]) %>%
  wilcox_test(as.formula(paste("Shannon ~", INNER.VAR, sep = " "))) %>%
  adjust_pvalue(method = "BH") %>%
  add_significance("p.adj") %>% 
  add_xy_position(x = OUTER.VAR, dodge = 0.8) %>%
  p_format("p.adj", accuracy = 0.0001, trailing.zero = TRUE, new.col = TRUE)

## Calculate stats for outer variable
# Perform pairwise comparisons
stat.test2 <- dat %>%
  wilcox_test(as.formula(paste("Shannon ~", OUTER.VAR, sep = " "))) %>%
  adjust_pvalue(method = "BH") %>%
  add_significance("p.adj") %>% 
  add_xy_position(x = OUTER.VAR) %>%
  p_format("p.adj", accuracy = 0.0001, trailing.zero = TRUE, new.col = TRUE)

# Adjust y value for outer p-values
stat.test2$y.position <- max(stat.test$y.position)*1.1

# Create plot
p <- ggboxplot(dat, x = OUTER.VAR, y = "Shannon",
          color = INNER.VAR, palette = "jco",
          add = "jitter")

# Add p-values
p.sha <- p + stat_pvalue_manual(stat.test, label = "p.adj.format",tip.length = 0)
p.sha <- p.sha + stat_pvalue_manual(stat.test2, label = "p.adj.format",tip.length = 0.02, step.increase = 0.1)

### Create output plot
filename <- paste0("plots/adiv_",OUTER.VAR,"_",INNER.VAR,".png")
adiv_plot <- ggarrange(p.obs,p.sha, nrow = 1, labels = c("A)","B)"), common.legend = TRUE,legend = "bottom")
adiv_plot + ggtitle(paste0("Difference in alpha diversity between ",OUTER.VAR, " and ",INNER.VAR,":"))
suppressMessages(ggsave(filename = filename, plot = adiv_plot, device = "png"))


# clear the environment and release memory
rm(list = ls(all.names = TRUE)[ls(all.names = TRUE) != "params"])
invisible(gc())


```

## CONTIOUS VARIABLES EFFECTS

When testing contious variables I will recommend to first test for correlations between all relevant variables. If two or more variables are strongly correlated it is not necessary to test them all. Any identified batch effects should still be kept in mind when testing continous variables. Also it can be relevant to include information about categorical variables in the analysis. The procedure is based on the suggestions from Statistical tools for high-throughput data analysis ([STHDA](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r))

### THEORY {.tabset .tabset-fade .tabset-pills}

#### SHORT VERSION

The theory behind statistical testing of correlations between two continous variables is a bit complex. It helps explain why the analysis is structured as it is, but it should not be necessary to read it, unless you have a special interest in statistical theory.

The following tabs summarise the information from the above mentioned website.

#### Methods for correlation analyses

There are different methods to perform **correlation analysis**:

##### Pearson correlation (r)

Pearson correlation measures a linear dependence between two variables (x and y). It's also known as a **parametric correlation** test because it depends to the distribution of the data. It can be used only when x and y are from normal distribution. The plot of $y=f(x)$ is named the **linear regression** curve.

##### Kendall rank correlation test

The Kendall rank correlation coefficient or Kendall's tau $(\tau)$ statistic is used to estimate a rank-based measure of association. This test may be used if the data do not necessarily come from a bivariate normal distribution.

##### Spearman rank correlation coefficient

Spearman's rho $(\rho)$ statistic is also used to estimate a rank-based measure of association. This test may be used if the data do not come from a bivariate normal distribution.

#### Correlation formula

In the formula below,

-   **x** and **y** are two vectors of length **n**

-   mxmx and mymy corresponds to the means of x and y, respectively.

**Pearson correlation formula**

$$\frac{\sum(x−m_{x})(y−m_{y})∑(x−mx)}{\sqrt{\sum(x−m_{x})^2\sum(y−m_{y})^2}}$$

$m_{x}$ and $m_{y}$ are the means of x and y variables.

The p-value (significance level) of the correlation can be determined :

1.  by using the correlation coefficient table for the degrees of freedom : $df=n−2$, where nn is the number of observation in x and y variables.

2.  or by calculating the **t value** as follow:

$$t = \frac{r}{\sqrt{1-r^2}}\sqrt{n-2} $$ In the case 2) the corresponding p-value is determined using [**t distribution table**](http://www.sthda.com/english/wiki/t-distribution-table) for $df=n−2$

**Spearman correlation formula** The Spearman correlation method computes the correlation between the rank of $x$ and the rank of $y$ variables. $$\rho=\frac{\sum(x'-m_{x'})(y'_{i}-m_{y'})}{\sqrt{\sum(x'-m_{x'})^2\sum(y'_{i}-m_{y'})^2}}$$ where $x'=rank(x)$ and $y'=rank(y)$.

**Kendall correlation formula** The Kendall correlation method measures the correspondence between the ranking of x and y variables. The total number of possible pairings of x with y observations is $n(n−1)/2$, where n is the size of x and y.

The procedure is as follow: - Begin by ordering the pairs by the x values. If x and y are correlated, then they would have the same relative rank orders. - Now, for each $y_{i}$, count the number of $y_{j}>y_{i}$ (**concordant pairs (c)**) and the number of $y_{j}<y_{i}$ (**discordant pairs (d)**).

**Kendall correlation distance** is defined as follow: $$\tau=\frac{n_{c}-n_{d}}{\frac{1}{2}n(n-1)}$$ Where, - $n_{c}$: total number of concordant pairs - $n_{d}$: total number of discordant pairs - $n$: size of x and y

### INDEPENDENCY OF VARIABLES

When testing contious variables I will recommend to first test for correlations between all relevant variables. If two or more variables are strongly correlated they should not all be tested.

```{r cor_variables, eval=TRUE}

# Load data
load("R_objects/stat_test_dat.RData")

# create vector with the relevant variables (can also be subset by column indeces)
CON.VARS <- dat %>% select(where(is.numeric)) %>% colnames()

# Plot all variables against each other
pairs(dat[,CON.VARS], pch = 19,  cex = 0.5,
      lower.panel=NULL)

# Run Pearson test
(corrmat <- cor(dat[,CON.VARS], method = "pearson", use = "complete.obs"))

# Create heatmap
corrmat_rounded <- round(corrmat, 2)

melted_corrmat_rounded <- tibble(Var1 = rep(row.names(corrmat_rounded), length(row.names(corrmat_rounded))),
                                 Var2 = rep(row.names(corrmat_rounded), each = length(row.names(corrmat_rounded))),
                                 dist = as.numeric(matrix(corrmat_rounded)))

ggplot(melted_corrmat_rounded, aes(x = Var1, y = Var2, fill = dist)) + 
  labs(title = "Correlation between continous variables") + 
  geom_tile(color = "white") + 
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0, limit = c(-1,1), 
                      space = "Lab", name = "Correlation coefficient") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, size = 12, hjust = 1, vjust = 1), 
        axis.text.y = element_text(size = 12, hjust = 1, vjust = 0.5),
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        panel.grid.major = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        axis.ticks = element_blank()) + 
  geom_text(aes(x = Var1, y = Var2, label = dist), color = "black", size = 4) + 
  coord_fixed()

# The significance of any correlations can the be tested individually
cor_test(data = dat, vars = INDECES, )
with(dat, cor.test(Chao1,Observed, method = "pearson", use = "complete.obs"))
with(dat, cor.test(Chao1, DNA_Conc, method = "pearson", use = "complete.obs"))

# clear the environment and release memory
rm(list = ls(all.names = TRUE)[ls(all.names = TRUE) != "params"])
invisible(gc())
```

As we see that Observed, ACE, and Chao1 is strongly correlated there is no reason to test all of them so I will only test Observed richness.

### PERFORM CORRELATION TEST

First step is to determine if the variables are normally distributed.

```{r Adiv_prelim, eval=TRUE}

# Load data
load("R_objects/stat_test_dat.RData")

# store variables
INDEX <- "FaithPD"
VAR <- "DNA_Conc"

# Visualise data
ggscatter(dat, x = VAR, y = INDEX, # basic plot
          facet.by = params$batch, # This line can be commented out if alpha diversity is independent of batches
          add = "reg.line", conf.int = TRUE, # Adds a trendline with confidence interval
          cor.coef = TRUE, cor.method = "pearson") +  # performs a pearson test and show p-value 
  ggtitle("Preliminary visualization of data")

### Preliminary test of assumptions
## Create Q-Q plots
# Adiv plot
Q.ind <- ggqqplot(dat,x = INDEX)
# Variable plot
Q.var <- ggqqplot(dat, x = VAR)

filename <- paste0("plots/QQplot_",INDEX,"_",VAR,".png")
QQplot <- ggarrange(Q.ind,Q.var, nrow = 1, labels = c("A)","B)"), common.legend = TRUE,legend = "bottom")
QQplot + ggtitle(paste0("Q-Q plots for ",INDEX, " and ",VAR,":"))
suppressMessages(ggsave(filename = filename, plot = QQplot, device = "png"))

## Test for normal distribution using Shapiro test
# Shapiro-Wilk normality test for Adiv
shapiro.test(dat[,INDEX])
# Shapiro-Wilk normality test for Variable
shapiro.test(dat[,VAR])

```

To determine what test to use for the correlation analysis we must answer two questions:

1.  **Is the covariation linear?**

    -   In the situation where the scatter plots show curved patterns, we are dealing with nonlinear association between the two variables.

2.  **Are the data from each of the 2 variables (x, y) follow a normal distribution?** + **Visual inspection of the data normality using Q-Q plots (quantile-quantile plots)**.

    -   Q-Q plot draws the correlation between a given sample and the normal distribution. If the samples diverge from the trendline they are not normally distributed. + **From the Shapiro-Wilk normality test:** If the two p-values are greater than the significance level 0.05 it implies that the distribution of the data are not significantly different from normal distribution.

If it looks like the data follows the trendline in the preliminary plot we should test the correlation. If the test of normality determined that both variables are normally distributed we should use a **Pearson correlation** and if not we should use **Kendall correlation** or **Spearman correlation** (I prefer the latter)

```{r Adiv_continous_test}
# Set method
METHOD <- "spearman"

# Run test
cor.test(dat[,INDEX],dat[,VAR], method = METHOD)

# Create plot with correct correlation statistics
filename <- paste0("plots/CORplot_",INDEX,"_",VAR,".png")

ggscatter(dat, x = VAR, y = INDEX,
          facet.by = params$batch, # This line can be commented out if alpha diversity is independent of batches
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = METHOD)
suppressMessages(ggsave(filename = filename, device = "png"))

```

# SETTINGS {.tabset .tabset-fade .tabset-pills}

Overview of the parameters and packages that were used for this analysis

## PARAMETERS

The following paramenters were set in for this analysis:

```{r parameters, eval=TRUE}
params <- readRDS("R_objects/Adiv_params.RDS")

tmp <- unlist(params)
dat <- data.frame(Parameter = names(tmp), Value = unname(tmp))


kbl(dat, row.names = F) %>% kable_classic(lightable_options = "striped")

```

## SESSION INFO

The analysis was run in the following environment:

```{r packages, eval=TRUE}
sessionInfo()
```

\`\`\`
